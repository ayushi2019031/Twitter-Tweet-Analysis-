{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Analysis\n",
    "\n",
    "In this project, we will analyse the tweets with the most trending hashtag in New Delhi.\n",
    "\n",
    "So there are 3 steps involved in this process. \n",
    "\n",
    "1. Getting the data from Twitter\n",
    "\n",
    "2. Present the data in a proper format so that it can be analysed easily\n",
    "\n",
    "3. Analyse the data\n",
    "\n",
    "We will be working with python and Twitter API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from twitter\n",
    "\n",
    "\n",
    "I have used Twitter API tweepy for doing this. So, we will start with installing tweepy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make a developer account on twitter. You might need to wait for some time before it gets approved\n",
    " \n",
    "In the developer account click on \"Create a new App\" and you will be given credentials. \n",
    "\n",
    "Now , we will include those credentials to make GET requests from twitter api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'YBA2m8JJ9cy3E9wXxVi2yYcX8'\n",
    "consumer_secret= 'HFMeoWIOhDZAjbDpfgAyujxSSZEQI5BAOuGxPiQRT4HniPk9cq'\n",
    "access_token= '1342378688453648384-tVtbtEATUh3NpZWttbaoKFp438NXpI'\n",
    "access_token_secret= 'FTvj3KU0oJCe9ZJBGGnKDAMPhU6eICcqV7GqfUGu1a68Y'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api=tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to avoid running our program repetitively, we will want to store all the tweets in a file. So we will create a new text file \"json_dumps.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import tweepy\n",
    "import io\n",
    "\n",
    "File = io.open('json_dumps.txt', 'w', encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make the GET request. Note three things:\n",
    "\n",
    "1. We are using the hashtag \"cricket\" as, without retweets we can get more than 10000 tweets.  \n",
    "\n",
    "2. We will be using a filter for retweets(for uniqueness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data =  tweepy.Cursor(api.search,q=\"#cricket\"+\" -filter:retweets\",geocode=\"28.7041,77.1025,100km\", count=11000,lang=\"en\").items(11000)\n",
    "#tweet_data does extract the top trending hashtag win a particular location and works well but here we want a large number of tweets\n",
    "#(> 1000). So for now I have considered a really popular topic and location to be anywhere. \n",
    "\n",
    "\n",
    "tweet_data =  tweepy.Cursor(api.search,q=\"#cricket\"+\" -filter:retweets\", count=11000,lang=\"en\").items(11000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be storing the tweets in a csv file as well. For this we will use pandas.So , using open() we create a new csv file and then  import pandas and create a dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "csv_file = io.open(\"tweets_info.csv\", \"w\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we will iterate through all the tweets in the tweet_data object. Every tweet has a _json_ attribute to it that contains all the info about the tweet. So we will write the json strings to the text file and the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0;\n",
    "for tweet in  tweet_data:    \n",
    "    if (hasattr(tweet , \"_json\")):\n",
    "        i += 1;\n",
    "        File.write(json.dumps(tweet._json))\n",
    "        File.write(\"\\n\")\n",
    "        df_2 = pd.json_normalize(tweet._json)\n",
    "        df = df.append(df_2, ignore_index=True)\n",
    "        print(i)\n",
    "        if (i >=11000):\n",
    "            print(i)\n",
    "            break;\n",
    "export_csv = df.to_csv(csv_file, index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!! We have got all the tweets and we have got them in a suitable format too "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis of  the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got all the stuff in csv file. So let's start with making a wordcloud for all the tweets. \n",
    "Start with getting text of all the posts in the tweets. For this, to pevent the need of repititive computation, we will first store all the words of the posts in a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial approach was iterating through each tweet, extracting it's post text as a string and then split the string with spaces to get tokens. But the process is too slow and time taking. So for this, we will use spacy module in python.\n",
    "\n",
    "So start with installing it through pip on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required modules first. And then read the info stored in the tweets.text column in the csv file into a dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import io\n",
    "from spacy.lang.en import English\n",
    "# Use it on Jupyter Notebook or Google Colab\n",
    "# DIR_PATH = os.getcwd()\n",
    "# Use it on Python module\n",
    "DIR_PATH = os.path.dirname(\"__file__\")\n",
    "\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"tweet_info.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate through all rows in the dataframe and save all the individual words in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for i in range(11000):\n",
    "    first_dialogue = df.loc[i, \"text\"]\n",
    "    if first_dialogue == Nonedependency:\n",
    "        break;\n",
    "    # use spacy with the  parse \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    [str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\n",
    "    \n",
    "    # use spacy with the sentencizer\n",
    "    nlp = English()  # just the language with no model\n",
    "    sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "    nlp.add_pipe(sentencizer)\n",
    "    k = [str(sent) for sent in nlp(first_dialogue).sents]\n",
    "    for K in k:\n",
    "        final_list.append(K)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save this list in a text file for future analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"list_of_posts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write((' '.join([str(elem) for elem in final_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start with making a word cloud for all the posts related to the hashtag cricket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with importing all the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data of the posts in a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"list_of_posts.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the wordcloud and pyplot module to display the wordCloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "stopwords = set(STOPWORDS) \n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(data) \n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets analyse make a word cloud for the description of the users who tweeted related to cricket. We will plot histograms for the same. \n",
    "\n",
    "So we will repeat the same process as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import io\n",
    "from spacy.lang.en import English\n",
    "# Use it on Jupyter Notebook or Google Colab\n",
    "# DIR_PATH = os.getcwd()\n",
    "# Use it on Python module\n",
    "DIR_PATH = os.path.dirname(__file__)\n",
    "\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"tweet_info.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# Assign first_dialogue to the first row's \"Dialogue\" column\n",
    "final_list = []\n",
    "for i in range(1000):\n",
    "\tfirst_dialogue = df.loc[i, \"user.description\"]\n",
    "\tprint(i)\n",
    "\tprint(first_dialogue)\n",
    "\t# if first_dialogue == None:\n",
    "\t# \tbreak;\t\n",
    "\t# use spacy with the dependency parse \n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "\t#[str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\n",
    "\t\n",
    "\t# use spacy with the sentencizer\n",
    "\tnlp = English()  # just the language with no model\n",
    "\tsentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "\tnlp.add_pipe(sentencizer)\n",
    "\tfinal_list.append(first_dialogue)\n",
    "\t# try:\n",
    "\t# \tif (not(first_dialogue == nan)):\n",
    "\t# \t\tk = [str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\t# \t\tprint(k)\n",
    "\t# \t\tfor K in k:\n",
    "\t# \t\t\tfinal_list.append(K)\n",
    "\t# \t\tprint(i)\n",
    "\t# except:\n",
    "\t# \tprint(\"Ok\")\n",
    "with io.open(\"list_of_user_description.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write((' '.join([str(elem) for elem in final_list])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make the word cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "\n",
    "# importing all necessery modules \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import io\n",
    "\n",
    "# Reads 'Youtube04-Eminem.csv' file \n",
    "with io.open(\"list_of_user_description.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "comment_words = '' \n",
    "STOPWORDS.add(\"https\")\n",
    "STOPWORDS.add(\"RahulGandhi\")\n",
    "STOPWORDS.add(\"Rahul Gandhi\")\n",
    "STOPWORDS.add(\"Rahul\")\n",
    "STOPWORDS.add(\"will\")\n",
    "STOPWORDS.add(\"t\")\n",
    "STOPWORDS.add(\"co\")\n",
    "STOPWORDS.add(\"Please\")\n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "# # iterate through the csv file \n",
    "# for tokens in data: \n",
    "\t\n",
    "# \ttokens = tokens.lower()\n",
    "# \tprint(tokens) \n",
    "\t\n",
    "# \tcomment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "\t\t\t\tbackground_color ='white', \n",
    "\t\t\t\tstopwords = stopwords, \n",
    "\t\t\t\tmin_font_size = 10).generate(data) \n",
    "\n",
    "# plot the WordCloud image\t\t\t\t\t \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will analyse the sentiment of the posts(positive, negative and neutral) and will present the findings with a bar chart. We have used a textblob module for this.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import os\n",
    "import spacy\n",
    "import io\n",
    "from spacy.lang.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "# Use it on Jupyter Notebook or Google Colab\n",
    "# DIR_PATH = os.getcwd()\n",
    "# Use it on Python module\n",
    "DIR_PATH = os.path.dirname(__file__)\n",
    "\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"tweet_info.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# Assign first_dialogue to th\n",
    "df[\"polarity\"] =df['text'].apply(lambda tweet: TextBlob(tweet).polarity)\n",
    "\n",
    "print(df[\"polarity\"])\n",
    "\n",
    "positive  = 0\n",
    "negative = 0\n",
    "neutral = 0;\n",
    "\n",
    "for dub in df[\"polarity\"]:\n",
    "\tprint(dub)\n",
    "\tif dub > 0:\n",
    "\t\tpositive += 1;\n",
    "\tif (dub< 0):\n",
    "\t\tnegative += 1;\n",
    "\tif (dub == 0):\n",
    "\t\tneutral += 1;\n",
    "\n",
    "label1 = [\"positive\", \"negative\", \"neutral\"]\n",
    "label2 = [positive, negative, neutral]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.bar(label1, label2)\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "# a = np.array(df[\"user.followers_count\"]) \n",
    "  \n",
    "# # Creating histogram \n",
    "# fig, ax = plt.subplots(figsize =(10, 7),  \n",
    "#                         tight_layout = True) \n",
    "# ax.hist(a, bins = 1000 ,\n",
    "#                    color='#607c8e') \n",
    "  \n",
    "# # Show plot \n",
    "# plt.show() \n",
    "\n",
    "a = np.array(df[\"user.followers_count\"])\n",
    "\n",
    "print(df[\"user.followers_count\"]) \n",
    "# Creating histogram \n",
    "fig, ax = plt.subplots(figsize =(100, 70)) \n",
    "# cks = np.arange(0, 10000000, 100)x_ti\n",
    "# plt.xticks(x_ticks)\n",
    "ax.hist(a, bins = [0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000])\n",
    "ax.set_xticklabels([0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000], rotation=0, fontsize=10)\n",
    "\n",
    "\n",
    "# ax.set_xlabel('marks')\n",
    "# ax.set_ylabel('no. of students') \n",
    "\n",
    "\n",
    "# Show plot \n",
    "plt.show() \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "# a = np.array(df[\"user.followers_count\"]) \n",
    "  \n",
    "# # Creating histogram \n",
    "# fig, ax = plt.subplots(figsize =(10, 7),  \n",
    "#                         tight_layout = True) \n",
    "# ax.hist(a, bins = 1000 ,\n",
    "#                    color='#607c8e') \n",
    "  \n",
    "# # Show plot \n",
    "# plt.show() \n",
    "\n",
    "a = np.array(df[\"user.followers_count\"])\n",
    "\n",
    "print(df[\"user.friends_count\"]) \n",
    "# Creating histogram \n",
    "fig, ax = plt.subplots(figsize =(100, 70)) \n",
    "# cks = np.arange(0, 10000000, 100)x_ti\n",
    "# plt.xticks(x_ticks)\n",
    "ax.hist(a, bins = [0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000])\n",
    "ax.set_xticklabels([0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000], rotation=0, fontsize=10)\n",
    "\n",
    "\n",
    "# ax.set_xlabel('marks')\n",
    "# ax.set_ylabel('no. of students') \n",
    "\n",
    "\n",
    "# Show plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a wordCloud of the tags associated with cricket that have been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import io\n",
    "from spacy.lang.en import English\n",
    "# Use it on Jupyter Notebook or Google Colab\n",
    "# DIR_PATH = os.getcwd()\n",
    "# Use it on Python module\n",
    "DIR_PATH = os.path.dirname(__file__)\n",
    "\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"tweet_info.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# Assign first_dialogue to the first row's \"Dialogue\" column\n",
    "final_list = []\n",
    "for i in range(1000):\n",
    "\tfirst_dialogue = df.loc[i, \"entities.hashtags\"]\n",
    "\tprint(i)\n",
    "\tprint(first_dialogue)\n",
    "\t# if first_dialogue == None:\n",
    "\t# \tbreak;\t\n",
    "\t# use spacy with the dependency parse \n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "\t#[str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\n",
    "\t\n",
    "\t# use spacy with the sentencizer\n",
    "\tnlp = English()  # just the language with no model\n",
    "\tsentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "\tnlp.add_pipe(sentencizer)\n",
    "\tfinal_list.append(first_dialogue)\n",
    "\t# try:\n",
    "\t# \tif (not(first_dialogue == nan)):\n",
    "\t# \t\tk = [str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\t# \t\tprint(k)\n",
    "\t# \t\tfor K in k:\n",
    "\t# \t\t\tfinal_list.append(K)\n",
    "\t# \t\tprint(i)\n",
    "\t# except:\n",
    "\t# \tprint(\"Ok\")\n",
    "with io.open(\"list_of_hashtags_associated_with_tweets.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write((' '.join([str(elem) for elem in final_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use wordCloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "\n",
    "# importing all necessery modules \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import io\n",
    "\n",
    "# Reads 'Youtube04-Eminem.csv' file \n",
    "with io.open(\"list_of_hashtags_associated_with_tweets.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "comment_words = '' \n",
    "STOPWORDS.add(\"https\")\n",
    "STOPWORDS.add(\"RahulGandhi\")\n",
    "STOPWORDS.add(\"Rahul Gandhi\")\n",
    "STOPWORDS.add(\"Rahul\")\n",
    "STOPWORDS.add(\"will\")\n",
    "STOPWORDS.add(\"t\")\n",
    "STOPWORDS.add(\"co\")\n",
    "STOPWORDS.add(\"Please\")\n",
    "STOPWORDS.add(\"text\")\n",
    "STOPWORDS.add(\"indices\" '')\n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "# # iterate through the csv file \n",
    "# for tokens in data: \n",
    "\t\n",
    "# \ttokens = tokens.lower()\n",
    "# \tprint(tokens) \n",
    "\t\n",
    "# \tcomment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "\t\t\t\tbackground_color ='white', \n",
    "\t\t\t\tstopwords = stopwords, \n",
    "\t\t\t\tmin_font_size = 10).generate(data) \n",
    "\n",
    "# plot the WordCloud image\t\t\t\t\t \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand how often tweets related to cricket are retweeted, as while extracting the data, we were filtering out retweets to maintain uniqueness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to analyse users, lets first understand what is the distrubution of number of followers each user has vs distribution of number of people being followed by the users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "# a = np.array(df[\"user.followers_count\"]) \n",
    "  \n",
    "# # Creating histogram \n",
    "# fig, ax = plt.subplots(figsize =(10, 7),  \n",
    "#                         tight_layout = True) \n",
    "# ax.hist(a, bins = 1000 ,\n",
    "#                    color='#607c8e') \n",
    "  \n",
    "# # Show plot \n",
    "# plt.show() \n",
    "\n",
    "a = np.array(df[\"user.followers_count\"])\n",
    "\n",
    "print(df[\"user.followers_count\"]) \n",
    "# Creating histogram \n",
    "fig, ax = plt.subplots(figsize =(100, 70)) \n",
    "# cks = np.arange(0, 10000000, 100)x_ti\n",
    "# plt.xticks(x_ticks)\n",
    "ax.hist(a, bins = [0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000])\n",
    "ax.set_xticklabels([0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000], rotation=0, fontsize=10)\n",
    "\n",
    "\n",
    "# ax.set_xlabel('marks')\n",
    "# ax.set_ylabel('no. of students') \n",
    "\n",
    "\n",
    "# Show plot \n",
    "plt.show() \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# Creating dataset \n",
    "# a = np.array(df[\"user.followers_count\"]) \n",
    "  \n",
    "# # Creating histogram \n",
    "# fig, ax = plt.subplots(figsize =(10, 7),  \n",
    "#                         tight_layout = True) \n",
    "# ax.hist(a, bins = 1000 ,\n",
    "#                    color='#607c8e') \n",
    "  \n",
    "# # Show plot \n",
    "# plt.show() \n",
    "\n",
    "a = np.array(df[\"user.followers_count\"])\n",
    "\n",
    "print(df[\"user.friends_count\"]) \n",
    "# Creating histogram \n",
    "fig, ax = plt.subplots(figsize =(100, 70)) \n",
    "# cks = np.arange(0, 10000000, 100)x_ti\n",
    "# plt.xticks(x_ticks)\n",
    "ax.hist(a, bins = [0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000])\n",
    "ax.set_xticklabels([0, 250, 500, 1000,2500, 5000,10000,20000,30000,40000, 50000, 60000, 70000, 80000, 90000, 100000], rotation=0, fontsize=10)\n",
    "\n",
    "\n",
    "# ax.set_xlabel('marks')\n",
    "# ax.set_ylabel('no. of students') \n",
    "\n",
    "\n",
    "# Show plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a word cloud of description of users to understand what walks of life cricket tweeters come from :D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import io\n",
    "from spacy.lang.en import English\n",
    "# Use it on Jupyter Notebook or Google Colab\n",
    "# DIR_PATH = os.getcwd()\n",
    "# Use it on Python module\n",
    "DIR_PATH = os.path.dirname(__file__)\n",
    "\n",
    "FILE_PATH = os.path.join(DIR_PATH, \"tweet_info.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# Assign first_dialogue to the first row's \"Dialogue\" column\n",
    "final_list = []\n",
    "for i in range(1000):\n",
    "\tfirst_dialogue = df.loc[i, \"user.description\"]\n",
    "\tprint(i)\n",
    "\tprint(first_dialogue)\n",
    "\t# if first_dialogue == None:\n",
    "\t# \tbreak;\t\n",
    "\t# use spacy with the dependency parse \n",
    "\tnlp = spacy.load(\"en_core_web_sm\")\n",
    "\t#[str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\n",
    "\t\n",
    "\t# use spacy with the sentencizer\n",
    "\tnlp = English()  # just the language with no model\n",
    "\tsentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "\tnlp.add_pipe(sentencizer)\n",
    "\tfinal_list.append(first_dialogue)\n",
    "\t# try:\n",
    "\t# \tif (not(first_dialogue == nan)):\n",
    "\t# \t\tk = [str(sent) for sent in nlp(first_dialogue).sents]\n",
    "\t# \t\tprint(k)\n",
    "\t# \t\tfor K in k:\n",
    "\t# \t\t\tfinal_list.append(K)\n",
    "\t# \t\tprint(i)\n",
    "\t# except:\n",
    "\t# \tprint(\"Ok\")\n",
    "with io.open(\"list_of_user_description.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write((' '.join([str(elem) for elem in final_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "\n",
    "# importing all necessery modules \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import io\n",
    "\n",
    "# Reads 'Youtube04-Eminem.csv' file \n",
    "with io.open(\"list_of_user_description.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "\n",
    "comment_words = '' \n",
    "STOPWORDS.add(\"https\")\n",
    "STOPWORDS.add(\"RahulGandhi\")\n",
    "STOPWORDS.add(\"Rahul Gandhi\")\n",
    "STOPWORDS.add(\"Rahul\")\n",
    "STOPWORDS.add(\"will\")\n",
    "STOPWORDS.add(\"t\")\n",
    "STOPWORDS.add(\"co\")\n",
    "STOPWORDS.add(\"Please\")\n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "# # iterate through the csv file \n",
    "# for tokens in data: \n",
    "\t\n",
    "# \ttokens = tokens.lower()\n",
    "# \tprint(tokens) \n",
    "\t\n",
    "# \tcomment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "\t\t\t\tbackground_color ='white', \n",
    "\t\t\t\tstopwords = stopwords, \n",
    "\t\t\t\tmin_font_size = 10).generate(data) \n",
    "\n",
    "# plot the WordCloud image\t\t\t\t\t \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lers plot the retweeting frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
